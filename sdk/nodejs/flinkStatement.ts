// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "./types/input";
import * as outputs from "./types/output";
import * as utilities from "./utilities";

/**
 * [![General Availability](https://img.shields.io/badge/Lifecycle%20Stage-General%20Availability-%2345c6e8)](https://docs.confluent.io/cloud/current/api.html#section/Versioning/API-Lifecycle-Policy)
 *
 * > **Note:** It is recommended to set `lifecycle { preventDestroy = true }` on production instances to prevent accidental statement deletion. This setting rejects plans that would destroy or recreate the statement, such as attempting to change uneditable attributes. Read more about it in the Terraform docs.
 *
 * ## Example Usage
 *
 * ### Option #1: Manage multiple Flink Compute Pools in the same Pulumi Stack
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as confluentcloud from "@pulumi/confluentcloud";
 *
 * const randomIntTable = new confluentcloud.FlinkStatement("random_int_table", {
 *     organization: {
 *         id: main.id,
 *     },
 *     environment: {
 *         id: staging.id,
 *     },
 *     computePool: {
 *         id: example.id,
 *     },
 *     principal: {
 *         id: app_manager_flink.id,
 *     },
 *     statement: "CREATE TABLE random_int_table(ts TIMESTAMP_LTZ(3), random_value INT);",
 *     properties: {
 *         "sql.current-catalog": exampleConfluentEnvironment.displayName,
 *         "sql.current-database": exampleConfluentKafkaCluster.displayName,
 *     },
 *     restEndpoint: mainConfluentFlinkRegion.restEndpoint,
 *     credentials: {
 *         key: env_admin_flink_api_key.id,
 *         secret: env_admin_flink_api_key.secret,
 *     },
 * });
 * ```
 *
 * ### Option #2: Manage a single Flink Compute Pool in the same Pulumi Stack
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as confluentcloud from "@pulumi/confluentcloud";
 *
 * const example = new confluentcloud.FlinkStatement("example", {
 *     statement: "CREATE TABLE random_int_table(ts TIMESTAMP_LTZ(3), random_value INT);",
 *     properties: {
 *         "sql.current-catalog": confluentEnvironmentDisplayName,
 *         "sql.current-database": confluentKafkaClusterDisplayName,
 *     },
 * });
 * ```
 *
 * Example of `confluentcloud.FlinkStatement` that creates a model:
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as confluentcloud from "@pulumi/confluentcloud";
 *
 * const example = new confluentcloud.FlinkStatement("example", {
 *     statement: "CREATE MODEL `vector_encoding` INPUT (input STRING) OUTPUT (vector ARRAY<FLOAT>) WITH( 'TASK' = 'classification','PROVIDER' = 'OPENAI','OPENAI.ENDPOINT' = 'https://api.openai.com/v1/embeddings','OPENAI.API_KEY' = '{{sessionconfig/sql.secrets.openaikey}}');",
 *     properties: {
 *         "sql.current-catalog": confluentEnvironmentDisplayName,
 *         "sql.current-database": confluentKafkaClusterDisplayName,
 *     },
 *     propertiesSensitive: {
 *         "sql.secrets.openaikey": "***REDACTED***",
 *     },
 * });
 * ```
 *
 * ## Getting Started
 *
 * The following end-to-end example might help to get started with [Flink Statements](https://docs.confluent.io/cloud/current/flink/get-started/overview.html):
 *   * flink-quickstart
 *   * flink-carry-over-offset-between-statements
 *
 * ## Import
 *
 * You can import a Flink statement by using the Flink Statement name, for example:
 *
 * Option #1: Manage multiple Flink Compute Pools in the same Pulumi Stack
 *
 * ```sh
 * $ export IMPORT_CONFLUENT_ORGANIZATION_ID="<organization_id>"
 * $ export IMPORT_CONFLUENT_ENVIRONMENT_ID="<environment_id>"
 * $ export IMPORT_FLINK_COMPUTE_POOL_ID="<flink_compute_pool_id>"
 * $ export IMPORT_FLINK_API_KEY="<flink_api_key>"
 * $ export IMPORT_FLINK_API_SECRET="<flink_api_secret>"
 * $ export IMPORT_FLINK_REST_ENDPOINT="<flink_rest_endpoint>"
 * $ export IMPORT_FLINK_PRINCIPAL_ID="<flink_rest_endpoint>"
 * $ pulumi import confluentcloud:index/flinkStatement:FlinkStatement example cfeab4fe-b62c-49bd-9e99-51cc98c77a67
 * ```
 *
 * Option #2: Manage a single Flink Compute Pool in the same Pulumi Stack
 *
 * ```sh
 * $ pulumi import confluentcloud:index/flinkStatement:FlinkStatement example cfeab4fe-b62c-49bd-9e99-51cc98c77a67
 * ```
 *
 * !> **Warning:** Do not forget to delete terminal command history afterwards for security purposes.
 */
export class FlinkStatement extends pulumi.CustomResource {
    /**
     * Get an existing FlinkStatement resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: FlinkStatementState, opts?: pulumi.CustomResourceOptions): FlinkStatement {
        return new FlinkStatement(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'confluentcloud:index/flinkStatement:FlinkStatement';

    /**
     * Returns true if the given object is an instance of FlinkStatement.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is FlinkStatement {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === FlinkStatement.__pulumiType;
    }

    declare public readonly computePool: pulumi.Output<outputs.FlinkStatementComputePool>;
    /**
     * The Cluster API Credentials.
     */
    declare public readonly credentials: pulumi.Output<outputs.FlinkStatementCredentials | undefined>;
    declare public readonly environment: pulumi.Output<outputs.FlinkStatementEnvironment>;
    /**
     * (Optional String) The last Kafka offsets that a statement has processed. Represented by a mapping from Kafka topic to a string representation of partitions mapped to offsets. For example,
     * ```bash
     * "latest_offsets": {
     * "topic-1": "partition:0,offset:100;partition:1,offset:200",
     * "topic-2": "partition:0,offset:50"
     * }
     * ```
     */
    declare public /*out*/ readonly latestOffsets: pulumi.Output<{[key: string]: string}>;
    /**
     * (Optional String) The date and time at which the Kafka topic offsets were added to the statement status. It is represented in RFC3339 format and is in UTC. For example, `2023-03-31T00:00:00-00:00`.
     */
    declare public /*out*/ readonly latestOffsetsTimestamp: pulumi.Output<string>;
    declare public readonly organization: pulumi.Output<outputs.FlinkStatementOrganization>;
    declare public readonly principal: pulumi.Output<outputs.FlinkStatementPrincipal>;
    /**
     * The custom topic settings to set:
     */
    declare public readonly properties: pulumi.Output<{[key: string]: string}>;
    /**
     * Block for sensitive statement properties:
     */
    declare public readonly propertiesSensitive: pulumi.Output<{[key: string]: string}>;
    /**
     * The REST endpoint of the Flink region. For example, for public networking: `https://flink.us-east-1.aws.confluent.cloud`. In the case of private networking, the endpoint might look like `https://flink.pr1jy6.us-east-2.aws.confluent.cloud`. You can construct it using either:
     * - `data.confluent_flink_region.main.private_rest_endpoint`, or
     * - `https://flink${data.confluent_network.main.endpoint_suffix}`
     */
    declare public readonly restEndpoint: pulumi.Output<string | undefined>;
    /**
     * The raw SQL text statement, for example, `SELECT CURRENT_TIMESTAMP;`.
     */
    declare public readonly statement: pulumi.Output<string>;
    /**
     * The ID of the Flink Statement, for example, `cfeab4fe-b62c-49bd-9e99-51cc98c77a67`.
     */
    declare public readonly statementName: pulumi.Output<string>;
    /**
     * The boolean flag is used to indicate the statement's running status and to control whether the Flink Statement should be stopped or resumed. Defaults to `false`. Update it to `true` to stop the statement. Subsequently update it to `false` to resume the statement.
     *
     * !> **Note:** To stop a running statement, no other argument can be updated except `stopped`.
     *
     * !> **Note:** When resuming a stopped statement, you can update `principal.id` and/or `compute_pool.id` in addition to `stopped` attribute. This enables the statement to run under a different principal (with the appropriate role assignment) or a different Flink compute pool (as long as it is in the same Flink region as the original).
     *
     * !> **Note:** Currently, only 3 Flink statements support the resume feature, namely: `CREATE TABLE AS`, `INSERT INTO`, and `EXECUTE STATEMENT SET`.
     *
     * !> **Warning:** Use Option #2 to avoid exposing sensitive `credentials` value in a state file. When using Option #1, Terraform doesn't encrypt the sensitive `credentials` value of the `confluentcloud.FlinkStatement` resource, so you must keep your state file secure to avoid exposing it. Refer to the Terraform documentation to learn more about securing your state file.
     */
    declare public readonly stopped: pulumi.Output<boolean>;

    /**
     * Create a FlinkStatement resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: FlinkStatementArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: FlinkStatementArgs | FlinkStatementState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as FlinkStatementState | undefined;
            resourceInputs["computePool"] = state?.computePool;
            resourceInputs["credentials"] = state?.credentials;
            resourceInputs["environment"] = state?.environment;
            resourceInputs["latestOffsets"] = state?.latestOffsets;
            resourceInputs["latestOffsetsTimestamp"] = state?.latestOffsetsTimestamp;
            resourceInputs["organization"] = state?.organization;
            resourceInputs["principal"] = state?.principal;
            resourceInputs["properties"] = state?.properties;
            resourceInputs["propertiesSensitive"] = state?.propertiesSensitive;
            resourceInputs["restEndpoint"] = state?.restEndpoint;
            resourceInputs["statement"] = state?.statement;
            resourceInputs["statementName"] = state?.statementName;
            resourceInputs["stopped"] = state?.stopped;
        } else {
            const args = argsOrState as FlinkStatementArgs | undefined;
            if (args?.statement === undefined && !opts.urn) {
                throw new Error("Missing required property 'statement'");
            }
            resourceInputs["computePool"] = args?.computePool;
            resourceInputs["credentials"] = args?.credentials ? pulumi.secret(args.credentials) : undefined;
            resourceInputs["environment"] = args?.environment;
            resourceInputs["organization"] = args?.organization;
            resourceInputs["principal"] = args?.principal;
            resourceInputs["properties"] = args?.properties;
            resourceInputs["propertiesSensitive"] = args?.propertiesSensitive ? pulumi.secret(args.propertiesSensitive) : undefined;
            resourceInputs["restEndpoint"] = args?.restEndpoint;
            resourceInputs["statement"] = args?.statement;
            resourceInputs["statementName"] = args?.statementName;
            resourceInputs["stopped"] = args?.stopped;
            resourceInputs["latestOffsets"] = undefined /*out*/;
            resourceInputs["latestOffsetsTimestamp"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        const secretOpts = { additionalSecretOutputs: ["credentials", "propertiesSensitive"] };
        opts = pulumi.mergeOptions(opts, secretOpts);
        super(FlinkStatement.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering FlinkStatement resources.
 */
export interface FlinkStatementState {
    computePool?: pulumi.Input<inputs.FlinkStatementComputePool>;
    /**
     * The Cluster API Credentials.
     */
    credentials?: pulumi.Input<inputs.FlinkStatementCredentials>;
    environment?: pulumi.Input<inputs.FlinkStatementEnvironment>;
    /**
     * (Optional String) The last Kafka offsets that a statement has processed. Represented by a mapping from Kafka topic to a string representation of partitions mapped to offsets. For example,
     * ```bash
     * "latest_offsets": {
     * "topic-1": "partition:0,offset:100;partition:1,offset:200",
     * "topic-2": "partition:0,offset:50"
     * }
     * ```
     */
    latestOffsets?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * (Optional String) The date and time at which the Kafka topic offsets were added to the statement status. It is represented in RFC3339 format and is in UTC. For example, `2023-03-31T00:00:00-00:00`.
     */
    latestOffsetsTimestamp?: pulumi.Input<string>;
    organization?: pulumi.Input<inputs.FlinkStatementOrganization>;
    principal?: pulumi.Input<inputs.FlinkStatementPrincipal>;
    /**
     * The custom topic settings to set:
     */
    properties?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * Block for sensitive statement properties:
     */
    propertiesSensitive?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The REST endpoint of the Flink region. For example, for public networking: `https://flink.us-east-1.aws.confluent.cloud`. In the case of private networking, the endpoint might look like `https://flink.pr1jy6.us-east-2.aws.confluent.cloud`. You can construct it using either:
     * - `data.confluent_flink_region.main.private_rest_endpoint`, or
     * - `https://flink${data.confluent_network.main.endpoint_suffix}`
     */
    restEndpoint?: pulumi.Input<string>;
    /**
     * The raw SQL text statement, for example, `SELECT CURRENT_TIMESTAMP;`.
     */
    statement?: pulumi.Input<string>;
    /**
     * The ID of the Flink Statement, for example, `cfeab4fe-b62c-49bd-9e99-51cc98c77a67`.
     */
    statementName?: pulumi.Input<string>;
    /**
     * The boolean flag is used to indicate the statement's running status and to control whether the Flink Statement should be stopped or resumed. Defaults to `false`. Update it to `true` to stop the statement. Subsequently update it to `false` to resume the statement.
     *
     * !> **Note:** To stop a running statement, no other argument can be updated except `stopped`.
     *
     * !> **Note:** When resuming a stopped statement, you can update `principal.id` and/or `compute_pool.id` in addition to `stopped` attribute. This enables the statement to run under a different principal (with the appropriate role assignment) or a different Flink compute pool (as long as it is in the same Flink region as the original).
     *
     * !> **Note:** Currently, only 3 Flink statements support the resume feature, namely: `CREATE TABLE AS`, `INSERT INTO`, and `EXECUTE STATEMENT SET`.
     *
     * !> **Warning:** Use Option #2 to avoid exposing sensitive `credentials` value in a state file. When using Option #1, Terraform doesn't encrypt the sensitive `credentials` value of the `confluentcloud.FlinkStatement` resource, so you must keep your state file secure to avoid exposing it. Refer to the Terraform documentation to learn more about securing your state file.
     */
    stopped?: pulumi.Input<boolean>;
}

/**
 * The set of arguments for constructing a FlinkStatement resource.
 */
export interface FlinkStatementArgs {
    computePool?: pulumi.Input<inputs.FlinkStatementComputePool>;
    /**
     * The Cluster API Credentials.
     */
    credentials?: pulumi.Input<inputs.FlinkStatementCredentials>;
    environment?: pulumi.Input<inputs.FlinkStatementEnvironment>;
    organization?: pulumi.Input<inputs.FlinkStatementOrganization>;
    principal?: pulumi.Input<inputs.FlinkStatementPrincipal>;
    /**
     * The custom topic settings to set:
     */
    properties?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * Block for sensitive statement properties:
     */
    propertiesSensitive?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The REST endpoint of the Flink region. For example, for public networking: `https://flink.us-east-1.aws.confluent.cloud`. In the case of private networking, the endpoint might look like `https://flink.pr1jy6.us-east-2.aws.confluent.cloud`. You can construct it using either:
     * - `data.confluent_flink_region.main.private_rest_endpoint`, or
     * - `https://flink${data.confluent_network.main.endpoint_suffix}`
     */
    restEndpoint?: pulumi.Input<string>;
    /**
     * The raw SQL text statement, for example, `SELECT CURRENT_TIMESTAMP;`.
     */
    statement: pulumi.Input<string>;
    /**
     * The ID of the Flink Statement, for example, `cfeab4fe-b62c-49bd-9e99-51cc98c77a67`.
     */
    statementName?: pulumi.Input<string>;
    /**
     * The boolean flag is used to indicate the statement's running status and to control whether the Flink Statement should be stopped or resumed. Defaults to `false`. Update it to `true` to stop the statement. Subsequently update it to `false` to resume the statement.
     *
     * !> **Note:** To stop a running statement, no other argument can be updated except `stopped`.
     *
     * !> **Note:** When resuming a stopped statement, you can update `principal.id` and/or `compute_pool.id` in addition to `stopped` attribute. This enables the statement to run under a different principal (with the appropriate role assignment) or a different Flink compute pool (as long as it is in the same Flink region as the original).
     *
     * !> **Note:** Currently, only 3 Flink statements support the resume feature, namely: `CREATE TABLE AS`, `INSERT INTO`, and `EXECUTE STATEMENT SET`.
     *
     * !> **Warning:** Use Option #2 to avoid exposing sensitive `credentials` value in a state file. When using Option #1, Terraform doesn't encrypt the sensitive `credentials` value of the `confluentcloud.FlinkStatement` resource, so you must keep your state file secure to avoid exposing it. Refer to the Terraform documentation to learn more about securing your state file.
     */
    stopped?: pulumi.Input<boolean>;
}
