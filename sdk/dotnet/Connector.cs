// *** WARNING: this file was generated by pulumi-language-dotnet. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.ConfluentCloud
{
    /// <summary>
    /// [![General Availability](https://img.shields.io/badge/Lifecycle%20Stage-General%20Availability-%2345c6e8)](https://docs.confluent.io/cloud/current/api.html#section/Versioning/API-Lifecycle-Policy)
    /// 
    /// `confluentcloud.Connector` provides a connector resource that enables creating, editing, and deleting connectors on Confluent Cloud.
    /// 
    /// &gt; **Note:** Use [Confluent docs](https://docs.confluent.io/cloud/current/connectors/index.html) or the [Confluent Cloud Console](https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#using-the-ccloud-console) to pregenerate the configuration for your desired connector and to see what ACLs are required to be created.
    /// 
    /// &gt; **Note:** It is recommended to set `lifecycle { PreventDestroy = true }` on production instances to prevent accidental connector deletion. This setting rejects plans that would destroy or recreate the connector, such as attempting to change uneditable attributes. Read more about it in the Terraform docs.
    /// 
    /// ## Example Usage
    /// 
    /// ### Example Managed [Datagen Source Connector](https://docs.confluent.io/cloud/current/connectors/cc-datagen-source.html) that uses a service account to communicate with your Kafka cluster
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using ConfluentCloud = Pulumi.ConfluentCloud;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     // https://github.com/confluentinc/terraform-provider-confluent/tree/master/examples/configurations/connectors/managed-datagen-source-connector
    ///     var source = new ConfluentCloud.Connector("source", new()
    ///     {
    ///         Environment = new ConfluentCloud.Inputs.ConnectorEnvironmentArgs
    ///         {
    ///             Id = staging.Id,
    ///         },
    ///         KafkaCluster = new ConfluentCloud.Inputs.ConnectorKafkaClusterArgs
    ///         {
    ///             Id = basic.Id,
    ///         },
    ///         ConfigSensitive = null,
    ///         ConfigNonsensitive = 
    ///         {
    ///             { "connector.class", "DatagenSource" },
    ///             { "name", "DatagenSourceConnector_0" },
    ///             { "kafka.auth.mode", "SERVICE_ACCOUNT" },
    ///             { "kafka.service.account.id", app_connector.Id },
    ///             { "kafka.topic", orders.TopicName },
    ///             { "output.data.format", "JSON" },
    ///             { "quickstart", "ORDERS" },
    ///             { "tasks.max", "1" },
    ///         },
    ///     }, new CustomResourceOptions
    ///     {
    ///         DependsOn =
    ///         {
    ///             app_connector_describe_on_cluster,
    ///             app_connector_write_on_target_topic,
    ///             app_connector_create_on_data_preview_topics,
    ///             app_connector_write_on_data_preview_topics,
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ### Example Managed [Amazon S3 Sink Connector](https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html) that uses a service account to communicate with your Kafka cluster
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using ConfluentCloud = Pulumi.ConfluentCloud;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     // https://github.com/confluentinc/terraform-provider-confluent/tree/master/examples/configurations/connectors/s3-sink-connector
    ///     var sink = new ConfluentCloud.Connector("sink", new()
    ///     {
    ///         Environment = new ConfluentCloud.Inputs.ConnectorEnvironmentArgs
    ///         {
    ///             Id = staging.Id,
    ///         },
    ///         KafkaCluster = new ConfluentCloud.Inputs.ConnectorKafkaClusterArgs
    ///         {
    ///             Id = basic.Id,
    ///         },
    ///         ConfigSensitive = 
    ///         {
    ///             { "aws.access.key.id", "***REDACTED***" },
    ///             { "aws.secret.access.key", "***REDACTED***" },
    ///         },
    ///         ConfigNonsensitive = 
    ///         {
    ///             { "topics", orders.TopicName },
    ///             { "input.data.format", "JSON" },
    ///             { "connector.class", "S3_SINK" },
    ///             { "name", "S3_SINKConnector_0" },
    ///             { "kafka.auth.mode", "SERVICE_ACCOUNT" },
    ///             { "kafka.service.account.id", app_connector.Id },
    ///             { "s3.bucket.name", "&lt;s3-bucket-name&gt;" },
    ///             { "output.data.format", "JSON" },
    ///             { "time.interval", "DAILY" },
    ///             { "flush.size", "1000" },
    ///             { "tasks.max", "1" },
    ///         },
    ///     }, new CustomResourceOptions
    ///     {
    ///         DependsOn =
    ///         {
    ///             app_connector_describe_on_cluster,
    ///             app_connector_read_on_target_topic,
    ///             app_connector_create_on_dlq_lcc_topics,
    ///             app_connector_write_on_dlq_lcc_topics,
    ///             app_connector_create_on_success_lcc_topics,
    ///             app_connector_write_on_success_lcc_topics,
    ///             app_connector_create_on_error_lcc_topics,
    ///             app_connector_write_on_error_lcc_topics,
    ///             app_connector_read_on_connect_lcc_group,
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ### Example Managed [Amazon S3 Sink Connector](https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html) that uses a service account to communicate with your Kafka cluster and IAM Roles for AWS authentication
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using ConfluentCloud = Pulumi.ConfluentCloud;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     // https://github.com/confluentinc/terraform-provider-confluent/tree/master/examples/configurations/connectors/s3-sink-connector-assume-role
    ///     var sink = new ConfluentCloud.Connector("sink", new()
    ///     {
    ///         Environment = new ConfluentCloud.Inputs.ConnectorEnvironmentArgs
    ///         {
    ///             Id = staging.Id,
    ///         },
    ///         KafkaCluster = new ConfluentCloud.Inputs.ConnectorKafkaClusterArgs
    ///         {
    ///             Id = basic.Id,
    ///         },
    ///         ConfigSensitive = null,
    ///         ConfigNonsensitive = 
    ///         {
    ///             { "topics", orders.TopicName },
    ///             { "input.data.format", "JSON" },
    ///             { "connector.class", "S3_SINK" },
    ///             { "name", "S3_SINKConnector_0" },
    ///             { "kafka.auth.mode", "SERVICE_ACCOUNT" },
    ///             { "kafka.service.account.id", app_connector.Id },
    ///             { "s3.bucket.name", "&lt;s3-bucket-name&gt;" },
    ///             { "output.data.format", "JSON" },
    ///             { "time.interval", "DAILY" },
    ///             { "flush.size", "1000" },
    ///             { "tasks.max", "1" },
    ///             { "authentication.method", "IAM Roles" },
    ///             { "provider.integration.id", main.Id },
    ///         },
    ///     }, new CustomResourceOptions
    ///     {
    ///         DependsOn =
    ///         {
    ///             app_connector_describe_on_cluster,
    ///             app_connector_read_on_target_topic,
    ///             app_connector_create_on_dlq_lcc_topics,
    ///             app_connector_write_on_dlq_lcc_topics,
    ///             app_connector_create_on_success_lcc_topics,
    ///             app_connector_write_on_success_lcc_topics,
    ///             app_connector_create_on_error_lcc_topics,
    ///             app_connector_write_on_error_lcc_topics,
    ///             app_connector_read_on_connect_lcc_group,
    ///             main,
    ///             s3AccessRole,
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ### Example Managed [Amazon DynamoDB Connector](https://docs.confluent.io/cloud/current/connectors/cc-amazon-dynamo-db-sink.html) that uses a service account to communicate with your Kafka cluster
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using ConfluentCloud = Pulumi.ConfluentCloud;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     // https://github.com/confluentinc/terraform-provider-confluent/tree/master/examples/configurations/connectors/dynamo-db-sink-connector
    ///     var sink = new ConfluentCloud.Connector("sink", new()
    ///     {
    ///         Environment = new ConfluentCloud.Inputs.ConnectorEnvironmentArgs
    ///         {
    ///             Id = staging.Id,
    ///         },
    ///         KafkaCluster = new ConfluentCloud.Inputs.ConnectorKafkaClusterArgs
    ///         {
    ///             Id = basic.Id,
    ///         },
    ///         ConfigSensitive = 
    ///         {
    ///             { "aws.access.key.id", "***REDACTED***" },
    ///             { "aws.secret.access.key", "***REDACTED***" },
    ///         },
    ///         ConfigNonsensitive = 
    ///         {
    ///             { "topics", orders.TopicName },
    ///             { "input.data.format", "JSON" },
    ///             { "connector.class", "DynamoDbSink" },
    ///             { "name", "DynamoDbSinkConnector_0" },
    ///             { "kafka.auth.mode", "SERVICE_ACCOUNT" },
    ///             { "kafka.service.account.id", app_connector.Id },
    ///             { "aws.dynamodb.pk.hash", "value.userid" },
    ///             { "aws.dynamodb.pk.sort", "value.pageid" },
    ///             { "tasks.max", "1" },
    ///         },
    ///     }, new CustomResourceOptions
    ///     {
    ///         DependsOn =
    ///         {
    ///             app_connector_describe_on_cluster,
    ///             app_connector_read_on_target_topic,
    ///             app_connector_create_on_dlq_lcc_topics,
    ///             app_connector_write_on_dlq_lcc_topics,
    ///             app_connector_create_on_success_lcc_topics,
    ///             app_connector_write_on_success_lcc_topics,
    ///             app_connector_create_on_error_lcc_topics,
    ///             app_connector_write_on_error_lcc_topics,
    ///             app_connector_read_on_connect_lcc_group,
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ### Example Managed [MySQL Sink Connector](https://docs.confluent.io/cloud/current/connectors/cc-mysql-sink.html) that uses a service account to communicate with your Kafka cluster
    /// 
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using ConfluentCloud = Pulumi.ConfluentCloud;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     // https://github.com/confluentinc/terraform-provider-confluent/tree/master/examples/configurations/connectors/manage-offsets-source-sink-connector
    ///     var sink = new ConfluentCloud.Connector("sink", new()
    ///     {
    ///         Environment = new ConfluentCloud.Inputs.ConnectorEnvironmentArgs
    ///         {
    ///             Id = staging.Id,
    ///         },
    ///         KafkaCluster = new ConfluentCloud.Inputs.ConnectorKafkaClusterArgs
    ///         {
    ///             Id = basic.Id,
    ///         },
    ///         ConfigSensitive = 
    ///         {
    ///             { "connection.password", "***REDACTED***" },
    ///         },
    ///         ConfigNonsensitive = 
    ///         {
    ///             { "connector.class", "MySqlSink" },
    ///             { "name", "MySQLSinkConnector_0" },
    ///             { "topics", orders.TopicName },
    ///             { "input.data.format", "AVRO" },
    ///             { "kafka.auth.mode", "SERVICE_ACCOUNT" },
    ///             { "kafka.service.account.id", app_connector.Id },
    ///             { "db.name", "test_database" },
    ///             { "connection.user", "confluent_user" },
    ///             { "connection.host", "dev-testing-temp.abcdefghijk.us-west-7.rds.amazonaws.com" },
    ///             { "connection.port", "3306" },
    ///             { "insert.mode", "INSERT" },
    ///             { "auto.create", "true" },
    ///             { "auto.evolve", "true" },
    ///             { "tasks.max", "1" },
    ///         },
    ///         Offsets = new[]
    ///         {
    ///             new ConfluentCloud.Inputs.ConnectorOffsetArgs
    ///             {
    ///                 Partition = 
    ///                 {
    ///                     { "kafka_partition", "0" },
    ///                     { "kafka_topic", orders.TopicName },
    ///                 },
    ///                 Offset = 
    ///                 {
    ///                     { "kafka_offset", "100" },
    ///                 },
    ///             },
    ///             new ConfluentCloud.Inputs.ConnectorOffsetArgs
    ///             {
    ///                 Partition = 
    ///                 {
    ///                     { "kafka_partition", "1" },
    ///                     { "kafka_topic", orders.TopicName },
    ///                 },
    ///                 Offset = 
    ///                 {
    ///                     { "kafka_offset", "200" },
    ///                 },
    ///             },
    ///             new ConfluentCloud.Inputs.ConnectorOffsetArgs
    ///             {
    ///                 Partition = 
    ///                 {
    ///                     { "kafka_partition", "2" },
    ///                     { "kafka_topic", orders.TopicName },
    ///                 },
    ///                 Offset = 
    ///                 {
    ///                     { "kafka_offset", "300" },
    ///                 },
    ///             },
    ///         },
    ///     }, new CustomResourceOptions
    ///     {
    ///         DependsOn =
    ///         {
    ///             app_connector_describe_on_cluster,
    ///             app_connector_read_on_target_topic,
    ///             app_connector_create_on_dlq_lcc_topics,
    ///             app_connector_write_on_dlq_lcc_topics,
    ///             app_connector_create_on_success_lcc_topics,
    ///             app_connector_write_on_success_lcc_topics,
    ///             app_connector_create_on_error_lcc_topics,
    ///             app_connector_write_on_error_lcc_topics,
    ///             app_connector_read_on_connect_lcc_group,
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ### Example Custom [Datagen Source Connector](https://www.confluent.io/hub/confluentinc/kafka-connect-datagen) that uses a Kafka API Key to communicate with your Kafka cluster
    /// ```csharp
    /// using System.Collections.Generic;
    /// using System.Linq;
    /// using Pulumi;
    /// using ConfluentCloud = Pulumi.ConfluentCloud;
    /// 
    /// return await Deployment.RunAsync(() =&gt; 
    /// {
    ///     // https://github.com/confluentinc/terraform-provider-confluent/tree/master/examples/configurations/connectors/custom-datagen-source-connector
    ///     var source = new ConfluentCloud.Connector("source", new()
    ///     {
    ///         Environment = new ConfluentCloud.Inputs.ConnectorEnvironmentArgs
    ///         {
    ///             Id = staging.Id,
    ///         },
    ///         KafkaCluster = new ConfluentCloud.Inputs.ConnectorKafkaClusterArgs
    ///         {
    ///             Id = basic.Id,
    ///         },
    ///         ConfigSensitive = 
    ///         {
    ///             { "kafka.api.key", "***REDACTED***" },
    ///             { "kafka.api.secret", "***REDACTED***" },
    ///         },
    ///         ConfigNonsensitive = 
    ///         {
    ///             { "confluent.connector.type", "CUSTOM" },
    ///             { "connector.class", sourceConfluentCustomConnectorPlugin.ConnectorClass },
    ///             { "name", "DatagenConnectorExampleName" },
    ///             { "kafka.auth.mode", "KAFKA_API_KEY" },
    ///             { "kafka.topic", orders.TopicName },
    ///             { "output.data.format", "JSON" },
    ///             { "quickstart", "ORDERS" },
    ///             { "confluent.custom.plugin.id", sourceConfluentCustomConnectorPlugin.Id },
    ///             { "min.interval", "1000" },
    ///             { "max.interval", "2000" },
    ///             { "tasks.max", "1" },
    ///         },
    ///     }, new CustomResourceOptions
    ///     {
    ///         DependsOn =
    ///         {
    ///             app_manager_kafka_cluster_admin,
    ///         },
    ///     });
    /// 
    /// });
    /// ```
    /// 
    /// ## Getting Started
    /// 
    /// The following end-to-end examples might help to get started with `confluentcloud.Connector` resource:
    /// * s3-sink-connector
    /// * s3-sink-connector-assume-role
    /// * snowflake-sink-connector
    /// * managed-datagen-source-connector
    /// * elasticsearch-sink-connector
    /// * dynamo-db-sink-connector
    /// * mongo-db-source-connector
    /// * mongo-db-sink-connector
    /// * sql-server-cdc-debezium-source-connector
    /// * postgre-sql-cdc-debezium-source-connector
    /// * custom-datagen-source-connector
    /// * manage-offsets-github-source-connector
    /// * manage-offsets-mongo-db-source-connector
    /// * manage-offsets-mysql-sink-connector
    /// 
    /// &gt; **Note:** Certain connectors require additional ACL entries. See [Additional ACL entries](https://docs.confluent.io/cloud/current/connectors/service-account.html#additional-acl-entries) for more details.
    /// 
    /// &gt; **Note:** If there isn’t an example available for your target connector in the connectors folder, use the Confluent Cloud Console to begin the provisioning process. This can help you determine the required ACLs (example). You can also refer to the connector’s documentation page ([example](https://docs.confluent.io/cloud/current/connectors/cc-amazon-dynamo-db-sink.html#step-3-create-the-connector-configuration-file)) to copy its default configuration and learn more about all available settings. Afterward, we recommend selecting an example with the same connector type (sink or source) and modifying its ACL list and configuration to fit your specific requirements.
    /// 
    /// ## Import
    /// 
    /// &gt; **Note:** Set `ConfigSensitive = {}` and do not specify `Offsets` block before importing a connector.
    /// 
    /// You can import a connector by using Environment ID, Kafka cluster ID, and connector's name, in the format `&lt;Environment ID&gt;/&lt;Kafka cluster ID&gt;/&lt;Connector name&gt;`, for example:
    /// 
    /// ```sh
    /// $ export CONFLUENT_CLOUD_API_KEY="&lt;cloud_api_key&gt;"
    /// $ export CONFLUENT_CLOUD_API_SECRET="&lt;cloud_api_secret&gt;"
    /// $ pulumi import confluentcloud:index/connector:Connector my_connector "env-abc123/lkc-abc123/S3_SINKConnector_0"
    /// ```
    /// </summary>
    [ConfluentCloudResourceType("confluentcloud:index/connector:Connector")]
    public partial class Connector : global::Pulumi.CustomResource
    {
        /// <summary>
        /// Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        /// </summary>
        [Output("configNonsensitive")]
        public Output<ImmutableDictionary<string, string>> ConfigNonsensitive { get; private set; } = null!;

        /// <summary>
        /// Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        /// </summary>
        [Output("configSensitive")]
        public Output<ImmutableDictionary<string, string>> ConfigSensitive { get; private set; } = null!;

        /// <summary>
        /// Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        /// </summary>
        [Output("environment")]
        public Output<Outputs.ConnectorEnvironment> Environment { get; private set; } = null!;

        [Output("kafkaCluster")]
        public Output<Outputs.ConnectorKafkaCluster> KafkaCluster { get; private set; } = null!;

        /// <summary>
        /// Connector partitions with offsets
        /// </summary>
        [Output("offsets")]
        public Output<ImmutableArray<Outputs.ConnectorOffset>> Offsets { get; private set; } = null!;

        /// <summary>
        /// The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" &gt; "PAUSED"`) and resuming (`"PAUSED" &gt; "RUNNING"`) a connector is supported via an update operation.
        /// 
        /// For more information on connector offset management, see [Manage Offsets for Fully-Managed Connectors in Confluent Cloud](https://docs.confluent.io/cloud/current/connectors/offsets.html).
        /// 
        /// &gt; **Note:** If there are no _sensitive_ configuration settings for your connector, set `ConfigSensitive = {}` explicitly.
        /// 
        /// &gt; **Note:** You may declare sensitive variables for secrets `ConfigSensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        /// </summary>
        [Output("status")]
        public Output<string> Status { get; private set; } = null!;


        /// <summary>
        /// Create a Connector resource with the given unique name, arguments, and options.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resource</param>
        /// <param name="args">The arguments used to populate this resource's properties</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public Connector(string name, ConnectorArgs args, CustomResourceOptions? options = null)
            : base("confluentcloud:index/connector:Connector", name, args ?? new ConnectorArgs(), MakeResourceOptions(options, ""))
        {
        }

        private Connector(string name, Input<string> id, ConnectorState? state = null, CustomResourceOptions? options = null)
            : base("confluentcloud:index/connector:Connector", name, state, MakeResourceOptions(options, id))
        {
        }

        private static CustomResourceOptions MakeResourceOptions(CustomResourceOptions? options, Input<string>? id)
        {
            var defaultOptions = new CustomResourceOptions
            {
                Version = Utilities.Version,
                AdditionalSecretOutputs =
                {
                    "configSensitive",
                },
            };
            var merged = CustomResourceOptions.Merge(defaultOptions, options);
            // Override the ID if one was specified for consistency with other language SDKs.
            merged.Id = id ?? merged.Id;
            return merged;
        }
        /// <summary>
        /// Get an existing Connector resource's state with the given name, ID, and optional extra
        /// properties used to qualify the lookup.
        /// </summary>
        ///
        /// <param name="name">The unique name of the resulting resource.</param>
        /// <param name="id">The unique provider ID of the resource to lookup.</param>
        /// <param name="state">Any extra arguments used during the lookup.</param>
        /// <param name="options">A bag of options that control this resource's behavior</param>
        public static Connector Get(string name, Input<string> id, ConnectorState? state = null, CustomResourceOptions? options = null)
        {
            return new Connector(name, id, state, options);
        }
    }

    public sealed class ConnectorArgs : global::Pulumi.ResourceArgs
    {
        [Input("configNonsensitive", required: true)]
        private InputMap<string>? _configNonsensitive;

        /// <summary>
        /// Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        /// </summary>
        public InputMap<string> ConfigNonsensitive
        {
            get => _configNonsensitive ?? (_configNonsensitive = new InputMap<string>());
            set => _configNonsensitive = value;
        }

        [Input("configSensitive")]
        private InputMap<string>? _configSensitive;

        /// <summary>
        /// Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        /// </summary>
        public InputMap<string> ConfigSensitive
        {
            get => _configSensitive ?? (_configSensitive = new InputMap<string>());
            set
            {
                var emptySecret = Output.CreateSecret(ImmutableDictionary.Create<string, string>());
                _configSensitive = Output.All(value, emptySecret).Apply(v => v[0]);
            }
        }

        /// <summary>
        /// Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        /// </summary>
        [Input("environment", required: true)]
        public Input<Inputs.ConnectorEnvironmentArgs> Environment { get; set; } = null!;

        [Input("kafkaCluster", required: true)]
        public Input<Inputs.ConnectorKafkaClusterArgs> KafkaCluster { get; set; } = null!;

        [Input("offsets")]
        private InputList<Inputs.ConnectorOffsetArgs>? _offsets;

        /// <summary>
        /// Connector partitions with offsets
        /// </summary>
        public InputList<Inputs.ConnectorOffsetArgs> Offsets
        {
            get => _offsets ?? (_offsets = new InputList<Inputs.ConnectorOffsetArgs>());
            set => _offsets = value;
        }

        /// <summary>
        /// The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" &gt; "PAUSED"`) and resuming (`"PAUSED" &gt; "RUNNING"`) a connector is supported via an update operation.
        /// 
        /// For more information on connector offset management, see [Manage Offsets for Fully-Managed Connectors in Confluent Cloud](https://docs.confluent.io/cloud/current/connectors/offsets.html).
        /// 
        /// &gt; **Note:** If there are no _sensitive_ configuration settings for your connector, set `ConfigSensitive = {}` explicitly.
        /// 
        /// &gt; **Note:** You may declare sensitive variables for secrets `ConfigSensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        /// </summary>
        [Input("status")]
        public Input<string>? Status { get; set; }

        public ConnectorArgs()
        {
        }
        public static new ConnectorArgs Empty => new ConnectorArgs();
    }

    public sealed class ConnectorState : global::Pulumi.ResourceArgs
    {
        [Input("configNonsensitive")]
        private InputMap<string>? _configNonsensitive;

        /// <summary>
        /// Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        /// </summary>
        public InputMap<string> ConfigNonsensitive
        {
            get => _configNonsensitive ?? (_configNonsensitive = new InputMap<string>());
            set => _configNonsensitive = value;
        }

        [Input("configSensitive")]
        private InputMap<string>? _configSensitive;

        /// <summary>
        /// Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        /// </summary>
        public InputMap<string> ConfigSensitive
        {
            get => _configSensitive ?? (_configSensitive = new InputMap<string>());
            set
            {
                var emptySecret = Output.CreateSecret(ImmutableDictionary.Create<string, string>());
                _configSensitive = Output.All(value, emptySecret).Apply(v => v[0]);
            }
        }

        /// <summary>
        /// Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        /// </summary>
        [Input("environment")]
        public Input<Inputs.ConnectorEnvironmentGetArgs>? Environment { get; set; }

        [Input("kafkaCluster")]
        public Input<Inputs.ConnectorKafkaClusterGetArgs>? KafkaCluster { get; set; }

        [Input("offsets")]
        private InputList<Inputs.ConnectorOffsetGetArgs>? _offsets;

        /// <summary>
        /// Connector partitions with offsets
        /// </summary>
        public InputList<Inputs.ConnectorOffsetGetArgs> Offsets
        {
            get => _offsets ?? (_offsets = new InputList<Inputs.ConnectorOffsetGetArgs>());
            set => _offsets = value;
        }

        /// <summary>
        /// The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" &gt; "PAUSED"`) and resuming (`"PAUSED" &gt; "RUNNING"`) a connector is supported via an update operation.
        /// 
        /// For more information on connector offset management, see [Manage Offsets for Fully-Managed Connectors in Confluent Cloud](https://docs.confluent.io/cloud/current/connectors/offsets.html).
        /// 
        /// &gt; **Note:** If there are no _sensitive_ configuration settings for your connector, set `ConfigSensitive = {}` explicitly.
        /// 
        /// &gt; **Note:** You may declare sensitive variables for secrets `ConfigSensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        /// </summary>
        [Input("status")]
        public Input<string>? Status { get; set; }

        public ConnectorState()
        {
        }
        public static new ConnectorState Empty => new ConnectorState();
    }
}
