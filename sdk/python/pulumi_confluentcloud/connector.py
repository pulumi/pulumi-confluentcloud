# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from . import _utilities
from . import outputs
from ._inputs import *

__all__ = ['ConnectorArgs', 'Connector']

@pulumi.input_type
class ConnectorArgs:
    def __init__(__self__, *,
                 config_nonsensitive: pulumi.Input[Mapping[str, pulumi.Input[str]]],
                 environment: pulumi.Input['ConnectorEnvironmentArgs'],
                 kafka_cluster: pulumi.Input['ConnectorKafkaClusterArgs'],
                 config_sensitive: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 status: Optional[pulumi.Input[str]] = None):
        """
        The set of arguments for constructing a Connector resource.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_nonsensitive: Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        :param pulumi.Input['ConnectorEnvironmentArgs'] environment: Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_sensitive: Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        :param pulumi.Input[str] status: The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" > "PAUSED"`) and resuming (`"PAUSED" > "RUNNING"`) a connector is supported via an update operation.
               
               > **Note:** If there are no _sensitive_ configuration settings for your connector, set `config_sensitive = {}` explicitly.
               
               > **Note:** You may declare sensitive variables for secrets `config_sensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        """
        pulumi.set(__self__, "config_nonsensitive", config_nonsensitive)
        pulumi.set(__self__, "environment", environment)
        pulumi.set(__self__, "kafka_cluster", kafka_cluster)
        if config_sensitive is not None:
            pulumi.set(__self__, "config_sensitive", config_sensitive)
        if status is not None:
            pulumi.set(__self__, "status", status)

    @property
    @pulumi.getter(name="configNonsensitive")
    def config_nonsensitive(self) -> pulumi.Input[Mapping[str, pulumi.Input[str]]]:
        """
        Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        """
        return pulumi.get(self, "config_nonsensitive")

    @config_nonsensitive.setter
    def config_nonsensitive(self, value: pulumi.Input[Mapping[str, pulumi.Input[str]]]):
        pulumi.set(self, "config_nonsensitive", value)

    @property
    @pulumi.getter
    def environment(self) -> pulumi.Input['ConnectorEnvironmentArgs']:
        """
        Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        """
        return pulumi.get(self, "environment")

    @environment.setter
    def environment(self, value: pulumi.Input['ConnectorEnvironmentArgs']):
        pulumi.set(self, "environment", value)

    @property
    @pulumi.getter(name="kafkaCluster")
    def kafka_cluster(self) -> pulumi.Input['ConnectorKafkaClusterArgs']:
        return pulumi.get(self, "kafka_cluster")

    @kafka_cluster.setter
    def kafka_cluster(self, value: pulumi.Input['ConnectorKafkaClusterArgs']):
        pulumi.set(self, "kafka_cluster", value)

    @property
    @pulumi.getter(name="configSensitive")
    def config_sensitive(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        """
        return pulumi.get(self, "config_sensitive")

    @config_sensitive.setter
    def config_sensitive(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "config_sensitive", value)

    @property
    @pulumi.getter
    def status(self) -> Optional[pulumi.Input[str]]:
        """
        The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" > "PAUSED"`) and resuming (`"PAUSED" > "RUNNING"`) a connector is supported via an update operation.

        > **Note:** If there are no _sensitive_ configuration settings for your connector, set `config_sensitive = {}` explicitly.

        > **Note:** You may declare sensitive variables for secrets `config_sensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        """
        return pulumi.get(self, "status")

    @status.setter
    def status(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "status", value)


@pulumi.input_type
class _ConnectorState:
    def __init__(__self__, *,
                 config_nonsensitive: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 config_sensitive: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 environment: Optional[pulumi.Input['ConnectorEnvironmentArgs']] = None,
                 kafka_cluster: Optional[pulumi.Input['ConnectorKafkaClusterArgs']] = None,
                 status: Optional[pulumi.Input[str]] = None):
        """
        Input properties used for looking up and filtering Connector resources.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_nonsensitive: Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_sensitive: Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        :param pulumi.Input['ConnectorEnvironmentArgs'] environment: Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        :param pulumi.Input[str] status: The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" > "PAUSED"`) and resuming (`"PAUSED" > "RUNNING"`) a connector is supported via an update operation.
               
               > **Note:** If there are no _sensitive_ configuration settings for your connector, set `config_sensitive = {}` explicitly.
               
               > **Note:** You may declare sensitive variables for secrets `config_sensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        """
        if config_nonsensitive is not None:
            pulumi.set(__self__, "config_nonsensitive", config_nonsensitive)
        if config_sensitive is not None:
            pulumi.set(__self__, "config_sensitive", config_sensitive)
        if environment is not None:
            pulumi.set(__self__, "environment", environment)
        if kafka_cluster is not None:
            pulumi.set(__self__, "kafka_cluster", kafka_cluster)
        if status is not None:
            pulumi.set(__self__, "status", status)

    @property
    @pulumi.getter(name="configNonsensitive")
    def config_nonsensitive(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        """
        return pulumi.get(self, "config_nonsensitive")

    @config_nonsensitive.setter
    def config_nonsensitive(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "config_nonsensitive", value)

    @property
    @pulumi.getter(name="configSensitive")
    def config_sensitive(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        """
        return pulumi.get(self, "config_sensitive")

    @config_sensitive.setter
    def config_sensitive(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "config_sensitive", value)

    @property
    @pulumi.getter
    def environment(self) -> Optional[pulumi.Input['ConnectorEnvironmentArgs']]:
        """
        Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        """
        return pulumi.get(self, "environment")

    @environment.setter
    def environment(self, value: Optional[pulumi.Input['ConnectorEnvironmentArgs']]):
        pulumi.set(self, "environment", value)

    @property
    @pulumi.getter(name="kafkaCluster")
    def kafka_cluster(self) -> Optional[pulumi.Input['ConnectorKafkaClusterArgs']]:
        return pulumi.get(self, "kafka_cluster")

    @kafka_cluster.setter
    def kafka_cluster(self, value: Optional[pulumi.Input['ConnectorKafkaClusterArgs']]):
        pulumi.set(self, "kafka_cluster", value)

    @property
    @pulumi.getter
    def status(self) -> Optional[pulumi.Input[str]]:
        """
        The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" > "PAUSED"`) and resuming (`"PAUSED" > "RUNNING"`) a connector is supported via an update operation.

        > **Note:** If there are no _sensitive_ configuration settings for your connector, set `config_sensitive = {}` explicitly.

        > **Note:** You may declare sensitive variables for secrets `config_sensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        """
        return pulumi.get(self, "status")

    @status.setter
    def status(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "status", value)


class Connector(pulumi.CustomResource):
    @overload
    def __init__(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 config_nonsensitive: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 config_sensitive: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 environment: Optional[pulumi.Input[pulumi.InputType['ConnectorEnvironmentArgs']]] = None,
                 kafka_cluster: Optional[pulumi.Input[pulumi.InputType['ConnectorKafkaClusterArgs']]] = None,
                 status: Optional[pulumi.Input[str]] = None,
                 __props__=None):
        """
        ## Example Usage
        ### Example Managed [Datagen Source Connector](https://docs.confluent.io/cloud/current/connectors/cc-datagen-source.html) that uses a service account to communicate with your Kafka cluster
        ```python
        import pulumi
        import pulumi_confluentcloud as confluentcloud

        source = confluentcloud.Connector("source",
            environment=confluentcloud.ConnectorEnvironmentArgs(
                id=confluent_environment["staging"]["id"],
            ),
            kafka_cluster=confluentcloud.ConnectorKafkaClusterArgs(
                id=confluent_kafka_cluster["basic"]["id"],
            ),
            config_sensitive={},
            config_nonsensitive={
                "connector.class": "DatagenSource",
                "name": "DatagenSourceConnector_0",
                "kafka.auth.mode": "SERVICE_ACCOUNT",
                "kafka.service.account.id": confluent_service_account["app-connector"]["id"],
                "kafka.topic": confluent_kafka_topic["orders"]["topic_name"],
                "output.data.format": "JSON",
                "quickstart": "ORDERS",
                "tasks.max": "1",
            },
            opts=pulumi.ResourceOptions(depends_on=[
                    confluent_kafka_acl["app-connector-describe-on-cluster"],
                    confluent_kafka_acl["app-connector-write-on-target-topic"],
                    confluent_kafka_acl["app-connector-create-on-data-preview-topics"],
                    confluent_kafka_acl["app-connector-write-on-data-preview-topics"],
                ]))
        ```
        ### Example Managed [Amazon S3 Sink Connector](https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html) that uses a service account to communicate with your Kafka cluster
        ```python
        import pulumi
        import pulumi_confluentcloud as confluentcloud

        sink = confluentcloud.Connector("sink",
            environment=confluentcloud.ConnectorEnvironmentArgs(
                id=confluent_environment["staging"]["id"],
            ),
            kafka_cluster=confluentcloud.ConnectorKafkaClusterArgs(
                id=confluent_kafka_cluster["basic"]["id"],
            ),
            config_sensitive={
                "aws.access.key.id": "***REDACTED***",
                "aws.secret.access.key": "***REDACTED***",
            },
            config_nonsensitive={
                "topics": confluent_kafka_topic["orders"]["topic_name"],
                "input.data.format": "JSON",
                "connector.class": "S3_SINK",
                "name": "S3_SINKConnector_0",
                "kafka.auth.mode": "SERVICE_ACCOUNT",
                "kafka.service.account.id": confluent_service_account["app-connector"]["id"],
                "s3.bucket.name": "<s3-bucket-name>",
                "output.data.format": "JSON",
                "time.interval": "DAILY",
                "flush.size": "1000",
                "tasks.max": "1",
            },
            opts=pulumi.ResourceOptions(depends_on=[
                    confluent_kafka_acl["app-connector-describe-on-cluster"],
                    confluent_kafka_acl["app-connector-read-on-target-topic"],
                    confluent_kafka_acl["app-connector-create-on-dlq-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-dlq-lcc-topics"],
                    confluent_kafka_acl["app-connector-create-on-success-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-success-lcc-topics"],
                    confluent_kafka_acl["app-connector-create-on-error-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-error-lcc-topics"],
                    confluent_kafka_acl["app-connector-read-on-connect-lcc-group"],
                ]))
        ```
        ### Example Managed [Amazon DynamoDB Connector](https://docs.confluent.io/cloud/current/connectors/cc-amazon-dynamo-db-sink.html) that uses a service account to communicate with your Kafka cluster
        ```python
        import pulumi
        import pulumi_confluentcloud as confluentcloud

        sink = confluentcloud.Connector("sink",
            environment=confluentcloud.ConnectorEnvironmentArgs(
                id=confluent_environment["staging"]["id"],
            ),
            kafka_cluster=confluentcloud.ConnectorKafkaClusterArgs(
                id=confluent_kafka_cluster["basic"]["id"],
            ),
            config_sensitive={
                "aws.access.key.id": "***REDACTED***",
                "aws.secret.access.key": "***REDACTED***",
            },
            config_nonsensitive={
                "topics": confluent_kafka_topic["orders"]["topic_name"],
                "input.data.format": "JSON",
                "connector.class": "DynamoDbSink",
                "name": "DynamoDbSinkConnector_0",
                "kafka.auth.mode": "SERVICE_ACCOUNT",
                "kafka.service.account.id": confluent_service_account["app-connector"]["id"],
                "aws.dynamodb.pk.hash": "value.userid",
                "aws.dynamodb.pk.sort": "value.pageid",
                "tasks.max": "1",
            },
            opts=pulumi.ResourceOptions(depends_on=[
                    confluent_kafka_acl["app-connector-describe-on-cluster"],
                    confluent_kafka_acl["app-connector-read-on-target-topic"],
                    confluent_kafka_acl["app-connector-create-on-dlq-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-dlq-lcc-topics"],
                    confluent_kafka_acl["app-connector-create-on-success-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-success-lcc-topics"],
                    confluent_kafka_acl["app-connector-create-on-error-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-error-lcc-topics"],
                    confluent_kafka_acl["app-connector-read-on-connect-lcc-group"],
                ]))
        ```
        ### Example Custom [Datagen Source Connector](https://www.confluent.io/hub/confluentinc/kafka-connect-datagen) that uses a Kafka API Key to communicate with your Kafka cluster

        ```python
        import pulumi
        import pulumi_confluentcloud as confluentcloud

        source = confluentcloud.Connector("source",
            environment=confluentcloud.ConnectorEnvironmentArgs(
                id=confluent_environment["staging"]["id"],
            ),
            kafka_cluster=confluentcloud.ConnectorKafkaClusterArgs(
                id=confluent_kafka_cluster["basic"]["id"],
            ),
            config_sensitive={
                "kafka.api.key": "***REDACTED***",
                "kafka.api.secret": "***REDACTED***",
            },
            config_nonsensitive={
                "confluent.connector.type": "CUSTOM",
                "connector.class": confluent_custom_connector_plugin["source"]["connector_class"],
                "name": "DatagenConnectorExampleName",
                "kafka.auth.mode": "KAFKA_API_KEY",
                "kafka.topic": confluent_kafka_topic["orders"]["topic_name"],
                "output.data.format": "JSON",
                "quickstart": "ORDERS",
                "confluent.custom.plugin.id": confluent_custom_connector_plugin["source"]["id"],
                "min.interval": "1000",
                "max.interval": "2000",
                "tasks.max": "1",
            },
            opts=pulumi.ResourceOptions(depends_on=[confluent_role_binding["app-manager-kafka-cluster-admin"]]))
        ```

        > **Note:** Custom connectors are available in **Preview** for early adopters. Preview features are introduced to gather customer feedback. This feature should be used only for evaluation and non-production testing purposes or to provide feedback to Confluent, particularly as it becomes more widely available in follow-on editions.\\
        **Preview** features are intended for evaluation use in development and testing environments only, and not for production use. The warranty, SLA, and Support Services provisions of your agreement with Confluent do not apply to Preview features. Preview features are considered to be a Proof of Concept as defined in the Confluent Cloud Terms of Service. Confluent may discontinue providing preview releases of the Preview features at any time in Confluent’s sole discretion.
        ## Getting Started

        The following end-to-end examples might help to get started with `Connector` resource:
        * `s3-sink-connector`
        * `snowflake-sink-connector`
        * `managed-datagen-source-connector`
        * `elasticsearch-sink-connector`
        * `dynamo-db-sink-connector`
        * `mongo-db-source-connector`
        * `mongo-db-sink-connector`
        * `sql-server-cdc-debezium-source-connector`
        * `postgre-sql-cdc-debezium-source-connector`
        * `custom-datagen-source-connector`

        > **Note:** Certain connectors require additional ACL entries. See [Additional ACL entries](https://docs.confluent.io/cloud/current/connectors/service-account.html#additional-acl-entries) for more details.

        ## Import

        You can import a connector by using Environment ID, Kafka cluster ID, and connector's name, in the format `<Environment ID>/<Kafka cluster ID>/<Connector name>`, for example:

         $ export CONFLUENT_CLOUD_API_KEY="<cloud_api_key>"

         $ export CONFLUENT_CLOUD_API_SECRET="<cloud_api_secret>"

        ```sh
        $ pulumi import confluentcloud:index/connector:Connector my_connector "env-abc123/lkc-abc123/S3_SINKConnector_0"
        ```

        :param str resource_name: The name of the resource.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_nonsensitive: Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_sensitive: Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        :param pulumi.Input[pulumi.InputType['ConnectorEnvironmentArgs']] environment: Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        :param pulumi.Input[str] status: The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" > "PAUSED"`) and resuming (`"PAUSED" > "RUNNING"`) a connector is supported via an update operation.
               
               > **Note:** If there are no _sensitive_ configuration settings for your connector, set `config_sensitive = {}` explicitly.
               
               > **Note:** You may declare sensitive variables for secrets `config_sensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        """
        ...
    @overload
    def __init__(__self__,
                 resource_name: str,
                 args: ConnectorArgs,
                 opts: Optional[pulumi.ResourceOptions] = None):
        """
        ## Example Usage
        ### Example Managed [Datagen Source Connector](https://docs.confluent.io/cloud/current/connectors/cc-datagen-source.html) that uses a service account to communicate with your Kafka cluster
        ```python
        import pulumi
        import pulumi_confluentcloud as confluentcloud

        source = confluentcloud.Connector("source",
            environment=confluentcloud.ConnectorEnvironmentArgs(
                id=confluent_environment["staging"]["id"],
            ),
            kafka_cluster=confluentcloud.ConnectorKafkaClusterArgs(
                id=confluent_kafka_cluster["basic"]["id"],
            ),
            config_sensitive={},
            config_nonsensitive={
                "connector.class": "DatagenSource",
                "name": "DatagenSourceConnector_0",
                "kafka.auth.mode": "SERVICE_ACCOUNT",
                "kafka.service.account.id": confluent_service_account["app-connector"]["id"],
                "kafka.topic": confluent_kafka_topic["orders"]["topic_name"],
                "output.data.format": "JSON",
                "quickstart": "ORDERS",
                "tasks.max": "1",
            },
            opts=pulumi.ResourceOptions(depends_on=[
                    confluent_kafka_acl["app-connector-describe-on-cluster"],
                    confluent_kafka_acl["app-connector-write-on-target-topic"],
                    confluent_kafka_acl["app-connector-create-on-data-preview-topics"],
                    confluent_kafka_acl["app-connector-write-on-data-preview-topics"],
                ]))
        ```
        ### Example Managed [Amazon S3 Sink Connector](https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html) that uses a service account to communicate with your Kafka cluster
        ```python
        import pulumi
        import pulumi_confluentcloud as confluentcloud

        sink = confluentcloud.Connector("sink",
            environment=confluentcloud.ConnectorEnvironmentArgs(
                id=confluent_environment["staging"]["id"],
            ),
            kafka_cluster=confluentcloud.ConnectorKafkaClusterArgs(
                id=confluent_kafka_cluster["basic"]["id"],
            ),
            config_sensitive={
                "aws.access.key.id": "***REDACTED***",
                "aws.secret.access.key": "***REDACTED***",
            },
            config_nonsensitive={
                "topics": confluent_kafka_topic["orders"]["topic_name"],
                "input.data.format": "JSON",
                "connector.class": "S3_SINK",
                "name": "S3_SINKConnector_0",
                "kafka.auth.mode": "SERVICE_ACCOUNT",
                "kafka.service.account.id": confluent_service_account["app-connector"]["id"],
                "s3.bucket.name": "<s3-bucket-name>",
                "output.data.format": "JSON",
                "time.interval": "DAILY",
                "flush.size": "1000",
                "tasks.max": "1",
            },
            opts=pulumi.ResourceOptions(depends_on=[
                    confluent_kafka_acl["app-connector-describe-on-cluster"],
                    confluent_kafka_acl["app-connector-read-on-target-topic"],
                    confluent_kafka_acl["app-connector-create-on-dlq-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-dlq-lcc-topics"],
                    confluent_kafka_acl["app-connector-create-on-success-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-success-lcc-topics"],
                    confluent_kafka_acl["app-connector-create-on-error-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-error-lcc-topics"],
                    confluent_kafka_acl["app-connector-read-on-connect-lcc-group"],
                ]))
        ```
        ### Example Managed [Amazon DynamoDB Connector](https://docs.confluent.io/cloud/current/connectors/cc-amazon-dynamo-db-sink.html) that uses a service account to communicate with your Kafka cluster
        ```python
        import pulumi
        import pulumi_confluentcloud as confluentcloud

        sink = confluentcloud.Connector("sink",
            environment=confluentcloud.ConnectorEnvironmentArgs(
                id=confluent_environment["staging"]["id"],
            ),
            kafka_cluster=confluentcloud.ConnectorKafkaClusterArgs(
                id=confluent_kafka_cluster["basic"]["id"],
            ),
            config_sensitive={
                "aws.access.key.id": "***REDACTED***",
                "aws.secret.access.key": "***REDACTED***",
            },
            config_nonsensitive={
                "topics": confluent_kafka_topic["orders"]["topic_name"],
                "input.data.format": "JSON",
                "connector.class": "DynamoDbSink",
                "name": "DynamoDbSinkConnector_0",
                "kafka.auth.mode": "SERVICE_ACCOUNT",
                "kafka.service.account.id": confluent_service_account["app-connector"]["id"],
                "aws.dynamodb.pk.hash": "value.userid",
                "aws.dynamodb.pk.sort": "value.pageid",
                "tasks.max": "1",
            },
            opts=pulumi.ResourceOptions(depends_on=[
                    confluent_kafka_acl["app-connector-describe-on-cluster"],
                    confluent_kafka_acl["app-connector-read-on-target-topic"],
                    confluent_kafka_acl["app-connector-create-on-dlq-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-dlq-lcc-topics"],
                    confluent_kafka_acl["app-connector-create-on-success-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-success-lcc-topics"],
                    confluent_kafka_acl["app-connector-create-on-error-lcc-topics"],
                    confluent_kafka_acl["app-connector-write-on-error-lcc-topics"],
                    confluent_kafka_acl["app-connector-read-on-connect-lcc-group"],
                ]))
        ```
        ### Example Custom [Datagen Source Connector](https://www.confluent.io/hub/confluentinc/kafka-connect-datagen) that uses a Kafka API Key to communicate with your Kafka cluster

        ```python
        import pulumi
        import pulumi_confluentcloud as confluentcloud

        source = confluentcloud.Connector("source",
            environment=confluentcloud.ConnectorEnvironmentArgs(
                id=confluent_environment["staging"]["id"],
            ),
            kafka_cluster=confluentcloud.ConnectorKafkaClusterArgs(
                id=confluent_kafka_cluster["basic"]["id"],
            ),
            config_sensitive={
                "kafka.api.key": "***REDACTED***",
                "kafka.api.secret": "***REDACTED***",
            },
            config_nonsensitive={
                "confluent.connector.type": "CUSTOM",
                "connector.class": confluent_custom_connector_plugin["source"]["connector_class"],
                "name": "DatagenConnectorExampleName",
                "kafka.auth.mode": "KAFKA_API_KEY",
                "kafka.topic": confluent_kafka_topic["orders"]["topic_name"],
                "output.data.format": "JSON",
                "quickstart": "ORDERS",
                "confluent.custom.plugin.id": confluent_custom_connector_plugin["source"]["id"],
                "min.interval": "1000",
                "max.interval": "2000",
                "tasks.max": "1",
            },
            opts=pulumi.ResourceOptions(depends_on=[confluent_role_binding["app-manager-kafka-cluster-admin"]]))
        ```

        > **Note:** Custom connectors are available in **Preview** for early adopters. Preview features are introduced to gather customer feedback. This feature should be used only for evaluation and non-production testing purposes or to provide feedback to Confluent, particularly as it becomes more widely available in follow-on editions.\\
        **Preview** features are intended for evaluation use in development and testing environments only, and not for production use. The warranty, SLA, and Support Services provisions of your agreement with Confluent do not apply to Preview features. Preview features are considered to be a Proof of Concept as defined in the Confluent Cloud Terms of Service. Confluent may discontinue providing preview releases of the Preview features at any time in Confluent’s sole discretion.
        ## Getting Started

        The following end-to-end examples might help to get started with `Connector` resource:
        * `s3-sink-connector`
        * `snowflake-sink-connector`
        * `managed-datagen-source-connector`
        * `elasticsearch-sink-connector`
        * `dynamo-db-sink-connector`
        * `mongo-db-source-connector`
        * `mongo-db-sink-connector`
        * `sql-server-cdc-debezium-source-connector`
        * `postgre-sql-cdc-debezium-source-connector`
        * `custom-datagen-source-connector`

        > **Note:** Certain connectors require additional ACL entries. See [Additional ACL entries](https://docs.confluent.io/cloud/current/connectors/service-account.html#additional-acl-entries) for more details.

        ## Import

        You can import a connector by using Environment ID, Kafka cluster ID, and connector's name, in the format `<Environment ID>/<Kafka cluster ID>/<Connector name>`, for example:

         $ export CONFLUENT_CLOUD_API_KEY="<cloud_api_key>"

         $ export CONFLUENT_CLOUD_API_SECRET="<cloud_api_secret>"

        ```sh
        $ pulumi import confluentcloud:index/connector:Connector my_connector "env-abc123/lkc-abc123/S3_SINKConnector_0"
        ```

        :param str resource_name: The name of the resource.
        :param ConnectorArgs args: The arguments to use to populate this resource's properties.
        :param pulumi.ResourceOptions opts: Options for the resource.
        """
        ...
    def __init__(__self__, resource_name: str, *args, **kwargs):
        resource_args, opts = _utilities.get_resource_args_opts(ConnectorArgs, pulumi.ResourceOptions, *args, **kwargs)
        if resource_args is not None:
            __self__._internal_init(resource_name, opts, **resource_args.__dict__)
        else:
            __self__._internal_init(resource_name, *args, **kwargs)

    def _internal_init(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 config_nonsensitive: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 config_sensitive: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 environment: Optional[pulumi.Input[pulumi.InputType['ConnectorEnvironmentArgs']]] = None,
                 kafka_cluster: Optional[pulumi.Input[pulumi.InputType['ConnectorKafkaClusterArgs']]] = None,
                 status: Optional[pulumi.Input[str]] = None,
                 __props__=None):
        opts = pulumi.ResourceOptions.merge(_utilities.get_resource_opts_defaults(), opts)
        if not isinstance(opts, pulumi.ResourceOptions):
            raise TypeError('Expected resource options to be a ResourceOptions instance')
        if opts.id is None:
            if __props__ is not None:
                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')
            __props__ = ConnectorArgs.__new__(ConnectorArgs)

            if config_nonsensitive is None and not opts.urn:
                raise TypeError("Missing required property 'config_nonsensitive'")
            __props__.__dict__["config_nonsensitive"] = config_nonsensitive
            __props__.__dict__["config_sensitive"] = None if config_sensitive is None else pulumi.Output.secret(config_sensitive)
            if environment is None and not opts.urn:
                raise TypeError("Missing required property 'environment'")
            __props__.__dict__["environment"] = environment
            if kafka_cluster is None and not opts.urn:
                raise TypeError("Missing required property 'kafka_cluster'")
            __props__.__dict__["kafka_cluster"] = kafka_cluster
            __props__.__dict__["status"] = status
        secret_opts = pulumi.ResourceOptions(additional_secret_outputs=["configSensitive"])
        opts = pulumi.ResourceOptions.merge(opts, secret_opts)
        super(Connector, __self__).__init__(
            'confluentcloud:index/connector:Connector',
            resource_name,
            __props__,
            opts)

    @staticmethod
    def get(resource_name: str,
            id: pulumi.Input[str],
            opts: Optional[pulumi.ResourceOptions] = None,
            config_nonsensitive: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
            config_sensitive: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
            environment: Optional[pulumi.Input[pulumi.InputType['ConnectorEnvironmentArgs']]] = None,
            kafka_cluster: Optional[pulumi.Input[pulumi.InputType['ConnectorKafkaClusterArgs']]] = None,
            status: Optional[pulumi.Input[str]] = None) -> 'Connector':
        """
        Get an existing Connector resource's state with the given name, id, and optional extra
        properties used to qualify the lookup.

        :param str resource_name: The unique name of the resulting resource.
        :param pulumi.Input[str] id: The unique provider ID of the resource to lookup.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_nonsensitive: Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] config_sensitive: Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        :param pulumi.Input[pulumi.InputType['ConnectorEnvironmentArgs']] environment: Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        :param pulumi.Input[str] status: The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" > "PAUSED"`) and resuming (`"PAUSED" > "RUNNING"`) a connector is supported via an update operation.
               
               > **Note:** If there are no _sensitive_ configuration settings for your connector, set `config_sensitive = {}` explicitly.
               
               > **Note:** You may declare sensitive variables for secrets `config_sensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        """
        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))

        __props__ = _ConnectorState.__new__(_ConnectorState)

        __props__.__dict__["config_nonsensitive"] = config_nonsensitive
        __props__.__dict__["config_sensitive"] = config_sensitive
        __props__.__dict__["environment"] = environment
        __props__.__dict__["kafka_cluster"] = kafka_cluster
        __props__.__dict__["status"] = status
        return Connector(resource_name, opts=opts, __props__=__props__)

    @property
    @pulumi.getter(name="configNonsensitive")
    def config_nonsensitive(self) -> pulumi.Output[Mapping[str, str]]:
        """
        Block for custom *nonsensitive* configuration properties that are *not* labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        """
        return pulumi.get(self, "config_nonsensitive")

    @property
    @pulumi.getter(name="configSensitive")
    def config_sensitive(self) -> pulumi.Output[Mapping[str, str]]:
        """
        Block for custom *sensitive* configuration properties that are labelled with "Type: password" under "Configuration Properties" section in [the docs](https://docs.confluent.io/cloud/current/connectors/index.html):
        """
        return pulumi.get(self, "config_sensitive")

    @property
    @pulumi.getter
    def environment(self) -> pulumi.Output['outputs.ConnectorEnvironment']:
        """
        Environment objects represent an isolated namespace for your Confluent resources for organizational purposes.
        """
        return pulumi.get(self, "environment")

    @property
    @pulumi.getter(name="kafkaCluster")
    def kafka_cluster(self) -> pulumi.Output['outputs.ConnectorKafkaCluster']:
        return pulumi.get(self, "kafka_cluster")

    @property
    @pulumi.getter
    def status(self) -> pulumi.Output[str]:
        """
        The status of the connector (one of `"NONE"`, `"PROVISIONING"`, `"RUNNING"`, `"DEGRADED"`, `"FAILED"`, `"PAUSED"`, `"DELETED"`). Pausing (`"RUNNING" > "PAUSED"`) and resuming (`"PAUSED" > "RUNNING"`) a connector is supported via an update operation.

        > **Note:** If there are no _sensitive_ configuration settings for your connector, set `config_sensitive = {}` explicitly.

        > **Note:** You may declare sensitive variables for secrets `config_sensitive` block and set them using environment variables (for example, `export TF_VAR_aws_access_key_id="foo"`).
        """
        return pulumi.get(self, "status")

